# Notes-Text-Summarization

Date 2020.2
For MLP Course Project with [@Hzlvv](https://github.com/Hzlvv)


## 阅读

### 论文

Training Language GANs from Scratch https://arxiv.org/pdf/1905.09922.pdf



Towards Explainable NLP: A Generative Explanation Framework for Text Classification https://www.aclweb.org/anthology/P19-1560.pdf



Using Semantic Similarity as Reward for Reinforcement Learning in Sentence Generation https://www.aclweb.org/anthology/P19-2056.pdf



Adversarial Learning for Neural Dialogue Generation https://arxiv.org/pdf/1701.06547.pdf

Sequence-to-Sequence Generative Argumentative Dialogue Systems with Self-Attention https://web.stanford.edu/class/cs224n/reports/custom/15844523.pdf



Generating Logical Forms from Graph Representations of Text and Entities https://www.aclweb.org/anthology/P19-1010.pdf



Synthetic QA Corpora Generation with Roundtrip Consistency https://www.aclweb.org/anthology/P19-1620.pdf



Keeping Notes: Conditional Natural Language Generation with a Scratchpad Mechanism https://arxiv.org/pdf/1906.05275.pdf



This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation https://www.aclweb.org/anthology/P19-1043.pdf

Strategies for Structuring Story Generation https://www.aclweb.org/anthology/P19-1254.pdf

Generating Summaries with Topic Templates and Structured Convolutional Decoders https://www.aclweb.org/anthology/P19-1504.pdf

Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization https://www.aclweb.org/anthology/D18-1206.pdf

HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization https://www.aclweb.org/anthology/P19-1499.pdf

Fine-tune BERT for Extractive Summarization https://arxiv.org/pdf/1903.10318.pdf

Ranking Sentences for Extractive Summarization with Reinforcement Learning https://arxiv.org/pdf/1802.08636.pdf

Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model https://www.aclweb.org/anthology/P19-1102.pdf

A Simple Theoretical Model of Importance for Summarization https://www.aclweb.org/anthology/P19-1101.pdf

Text Summarization with Pretrained Encoders https://www.aclweb.org/anthology/D19-1387.pdf

PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization [Google] https://arxiv.org/pdf/1912.08777.pdf

Leveraging BERT for Extractive Text Summarization on Lectures https://arxiv.org/pdf/1906.04165.pdf

Text Summarization Method based on Double Attention Pointer Network https://www.researchgate.net/publication/338521721_Text_Summarization_Method_based_on_Double_Attention_Pointer_Network

Universal Language Model Fine-tuning for Text Classification https://arxiv.org/pdf/1801.06146.pdf

### 其他

#### NLP非paper

- 一份从入门到精通NLP的完整指南 ｜ NLPer
  - https://mp.weixin.qq.com/s/AzLvUhN0aUdRIUmUcNUApw
  - 【关键字】NLP 指南 大全

- NLP的12种后BERT预训练方法
  - https://mp.weixin.qq.com/s/mkDmn4zy_s87kiiDIkx0VQ
  - RoBERTa ERNIE ERNIE 2.0 XLMs MASS UNILM CMLM ELECTRA SpanBERT ALBERT  MT-DNN XLENET

- 清华大学NLP组12份论文清单大礼包送给你!
  - https://mp.weixin.qq.com/s/Tf131lzPblx6D4oe73Pb0g
  - https://github.com/thunlp/LegalPapers

- 当深度学习遇见自动文本摘要
  - https://cloud.tencent.com/developer/article/1005548
- 干货 | 北京大学计算机科学技术研究所教授万小军：文本自动摘要技术
  -  https://mp.weixin.qq.com/s?__biz=MzAxMzc2NDAxOQ==&mid=2650363002&idx=2&sn=8c1f28d2944a373ae9133cb29bcd4c07
- 知识图谱如何助力文本摘要生成
  - https://mp.weixin.qq.com/s/TR9db8wpU7ooqZ6acKmrow
- Google Brain’s AI achieves state-of-the-art text summarization performance
  - https://venturebeat.com/2019/12/23/google-brains-ai-achieves-state-of-the-art-text-summarization-performance/
- The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) https://jalammar.github.io/illustrated-bert/
- 迁移学习NLP：BERT、ELMo等直观图解
  - https://zhuanlan.zhihu.com/p/52282552
- 【资源推荐】BERT论文合集
  - https://mp.weixin.qq.com/s/29Esivnb7sfjLT82sqR1Nw

#### 强化学习

- 强化学习在阿里的技术演进与业务创新 http://techforum-img.cn-hangzhou.oss-pub.aliyun-inc.com/1517812754285/reinforcement_learning.pdf

- RL in NMT: The Good, the Bad and the Ugly https://www.cl.uni-heidelberg.de/statnlpgroup/blog/rl4nmt/

- 机器翻译中的强化学习：优点、缺点以及不足 https://zhuanlan.zhihu.com/p/101324828

- Reinforcement Learning for NLP https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture16-guest.pdf

- 炼丹感悟：On the Generalization of RL https://zhuanlan.zhihu.com/p/105898705?utm_source=wechat_session&utm_medium=social&utm_oi=886328359599079424&wechatShare=1&s_s_i=6R6qXGoP1moxUv142VK6VTjNDy1osnvZhH5tUj9kQ5Q%3D&s_r=0&from=groupmessage&isappinstalled=0

####  模型压缩

- AdderNet: Do We Really Need Multiplications in Deep Learning? https://github.com/huawei-noah/AdderNet

## 数据集、任务

#### WIT

- WIT3 - acronym for Web Inventory of Transcribed and Translated Talks - is a ready-to-use version for research purposes of the multilingual transcriptions of [TED](http://www.ted.com/) talks
- https://wit3.fbk.eu/
- http://www.mt-archive.info/EAMT-2012-Cettolo.pdf

#### Multi30k

- Multi30K: Multilingual English-German Image Descriptions

- https://arxiv.org/pdf/1605.00459.pdf
- https://github.com/multi30k/dataset

#### Frames Dataset

- https://www.microsoft.com/en-us/research/project/frames-dataset/?secret=ZyeOOd#!stats
- The dialogues in Frames were collected in a Wizard-of-Oz fashion. Two humans talked to each other via a chat interface. 



#### The Blog Authorship Corpus 

- The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.  

  Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)

- http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm

- Effects of Age and Gender on Blogging http://u.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf

#### The Project Gutenberg collection

- The Project Gutenberg collection is available from dozens of sites offering access via http/https, ftp and/or rsync. See our listing of [mirror sites](https://www.gutenberg.org/MIRRORS.ALL) to choose the location closest to you. A close location may give you faster downloads. Mirrors generally do not have a friendly Web-based front end, but do have the collection. See the [mirroring how-to](https://www.gutenberg.org/wiki/Gutenberg:Mirroring_How-To) for details.
- https://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs

#### Enron

- This dataset was collected and prepared by the [CALO Project](http://www.ai.sri.com/project/CALO) (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 0.5M messages. This data was originally [made public, and posted to the web](http://www.salon.com/news/feature/2003/10/14/enron/index_np.html), by the [Federal Energy Regulatory Commission](http://www.ferc.gov/) during its investigation.
- https://www.cs.cmu.edu/~enron/

#### NEWSROOM

- **CORNELL NEWSROOM** is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017 and use a variety of summarization strategies combining *extraction* and *abstraction*.

- https://summari.es/
- https://github.com/lil-lab/newsroom
- [Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies](https://www.aclweb.org/anthology/N18-1065.pdf)

#### WikiHow

- A Large Scale Text Summarization Dataset
- WikiHow is a new large-scale dataset using the online WikiHow (http://www.wikihow.com/) knowledge base [*](https://github.com/mahnazkoupaee/WikiHow-Dataset#footnote1). 
- https://arxiv.org/abs/1810.09305

#### Reader-Aware Multi-Document Summarization (RA-MDS)

- Traditional multi-document summarization aims at generating a summary from a set of text documents, e.g., news, of a given topic/event. We investigate a problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.
- http://www1.se.cuhk.edu.hk/~textmine/dataset/ra-mds/

### 中文

#### LCSTS

- Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, we introduce a **L**arge-scale **C**hinese **S**hort **T**ext **S**ummarization dataset constructed from the Chinese microblogging website SinaWeibo. This corpus consists of over 2 million real Chinese short texts with short summaries given by the writer of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.

- [LCSTS: A Large Scale Chinese Short Text Summarization Dataset](https://www.aclweb.org/anthology/D15-1229.pdf)
- http://icrc.hitsz.edu.cn/Article/show/139.html

#### **NLPCC 2017 Shared Task**

-  **Single Document Summarization**
- http://tcci.ccf.org.cn/conference/2017/taskdata.php

#### 教育培训行业抽象式自动摘要中文语料库

- 教育行业新闻 自动文摘 语料库 自动摘要
- https://github.com/wonderfulsuccess/chinese_abstractive_corpus

#### chinese-poetry: 最全中文诗歌古典文集数据库

- 最全的中华古典文集数据库，包含 5.5 万首唐诗、26 万首宋诗、2.1 万首宋词和其他古典文集。诗人包括唐宋两朝近 1.4 万古诗人，和两宋时期 1.5 千古词人。数据来源于互联网。
- The most comprehensive database of Chinese poetry 🧶最全中华古诗词数据库, 唐宋两朝近一万四千古诗人, 接近5.5万首唐诗加26万宋诗. 两宋时期1564位词人，21050首词。
- [http://shici.store](http://shici.store/)
- https://github.com/chinese-poetry/chinese-poetry

### Source

数据集 - 阿里系唯一对外开放数据分享平台 https://tianchi.aliyun.com/dataset/?spm=5176.12281976.0.0.79166b2dgfWglv

Open Data on AWS https://registry.opendata.aws/

[ParlAI](https://github.com/facebookresearch/ParlAI) A framework for training and evaluating AI models on a variety of openly available dialogue dataset https://parl.ai/

NLDS Corpora https://nlds.soe.ucsc.edu/software [Natural Language and Dialogue Systems](https://nlds.soe.ucsc.edu/

[nlp-datasets](https://github.com/niderhoff/nlp-datasets)

中文长文本摘要有哪些数据集？ https://www.zhihu.com/question/306887936

中文数据集搜索 - 文本摘要 https://www.cluebenchmarks.com/dataSet_search_modify.html?keywords=%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81





## 模型

#### XLNet

- XLNet: Generalized Autoregressive Pretraining for Language Understanding

- https://github.com/zihangdai/xlnet
- https://arxiv.org/pdf/1906.08237.pdf
- 【什么是XLNet，它为什么比BERT效果好】https://zhuanlan.zhihu.com/p/107350079?utm_source=wechat_session&utm_medium=social&utm_oi=610548841527775232&s_r=0&from=groupmessage&isappinstalled=0
- 【20项任务上碾压BERT，CMU和Google大脑联合推出XLNet模型（全文翻译】https://mp.weixin.qq.com/s/rPhHwg2fILT28X2sdPH56A

#### ALBERT

- ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS

- https://github.com/google-research/ALBERT
- https://openreview.net/attachment?id=H1eA7AEtvS&name=original_pdf
- 【全面霸榜！Google推出轻量级BERT模型：ALBERT（全文翻译）】https://mp.weixin.qq.com/s/vuk8yK9E_DuWuoWJs3cijw

#### Text-To-Text Transfer Transformer - T5

- https://github.com/google-research/text-to-text-transfer-transformer
- https://arxiv.org/pdf/1910.10683.pdf
- 【突破迁移学习局限！谷歌提出“T5” 新NLP模型，多基准测试达SOTA】https://mp.weixin.qq.com/s/p3Mg5MKiCLaYQOWQ7NtcNA

#### ULMFiT

- Universal Language Model Fine-tuning for Text Classification https://arxiv.org/pdf/1801.06146.pdf
- 【2018迁移学习优秀论文-ULMFiT-面向文本分类的预训练模型】https://zhuanlan.zhihu.com/p/55348816

#### NEZHA

- NEZHA: NEURAL CONTEXTUALIZED REPRESENTATION FOR CHINESE LANGUAGE UNDERSTANDING
- 华为基于bert提出的中文预训练模型NEZHA中提出一种新的函数型的相对位置的position encoding的方法，并且说比目前的bert的方式更优
- https://arxiv.org/pdf/1909.00204.pdf

#### UNILM

- Microsoft Research

- Unified Language Model Pre-training for Natural Language Understanding and Generation
- https://arxiv.org/pdf/1905.03197.pdf

- https://github.com/microsoft/unilm

### Text Summarization

#### Xsum

- Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization https://www.aclweb.org/anthology/D18-1206.pdf

- https://github.com/EdinburghNLP/XSum
- https://vimeo.com/305885893

#### HIBERT

- HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization
- https://www.aclweb.org/anthology/P19-1499.pdf
- https://xingxingzhang.github.io/hibert.html

#### Presumm

- https://github.com/nlpyang/PreSumm
- Text Summarization with Pretrained Encoders https://www.aclweb.org/anthology/D19-1387.pdf
- Fine-tune BERT for Extractive Summarization https://arxiv.org/pdf/1903.10318.pdf



## 框架,实现,工具

- [dialogue-seq2seq](https://github.com/vliu15/dialogue-seq2seq)

  - Sequence-to-Sequence Generative Dialogue Systems. This is a Pytorch adaptation of the Transformer model in "[Attention is All You Need](https://arxiv.org/abs/1706.03762)" for memory-based generative dialogue systems. We borrow the Transformer encoder and decoder to encode decode individual responses. The encoded input updates a hidden state in an LSTM, which serves as a session memory. We train our dialogue system with the "[Internet Argument Corpus v1](https://nlds.soe.ucsc.edu/iac)".

- [bertviz](https://github.com/jessevig/bertviz)

  - BertViz is a tool for visualizing attention in the Transformer model, supporting all models from the [transformers](https://github.com/huggingface/transformers) library (BERT, GPT-2, XLNet, RoBERTa, XLM, CTRL, etc.). It extends the [Tensor2Tensor visualization tool](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization) by [Llion Jones](https://medium.com/@llionj) and the [transformers](https://github.com/huggingface/transformers) library from [HuggingFace](https://github.com/huggingface).

- [BERT-pytorch](https://github.com/codertimo/BERT-pytorch)

  - Pytorch implementation of Google AI's 2018 BERT, with simple annotation

- [transformers](https://github.com/huggingface/transformers)

  - 🤗 Transformers (formerly known as `pytorch-transformers` and `pytorch-pretrained-bert`) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.

  - 🤗 Transformers currently provides the following NLU/NLG architectures:

    1. **BERT** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.
    2. **GPT** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.
    3. **GPT-2** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.
    4. **Transformer-XL** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.
    5. **XLNet** (from Google/CMU) released with the paper [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.
    6. **XLM** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.
    7. **RoBERTa** (from Facebook), released together with the paper a [Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.
    8. **DistilBERT** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/master/examples/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/master/examples/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/master/examples/distillation) and a German version of DistilBERT.
    9. **CTRL** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.
    10. **CamemBERT** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.
    11. **ALBERT** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.
    12. **T5** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.
    13. **XLM-RoBERTa** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.
    14. **MMBT** (from Facebook), released together with the paper a [Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/pdf/1909.02950.pdf) by Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Davide Testuggine.
    15. **FlauBERT** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab.
    16. **Other community models**, contributed by the [community](https://huggingface.co/users).
    17. Want to contribute a new model? We have added a **detailed guide and templates** to guide you in the process of adding a new model. You can find them in the [`templates`](https://github.com/huggingface/transformers/blob/master/templates) folder of the repository. Be sure to check the [contributing guidelines](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md) and contact the maintainers or open an issue to collect feedbacks before starting your PR.

    These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the [documentation](https://huggingface.co/transformers/examples.html).

- Bert Extractive Summarizer 

  - This repo is the generalization of the lecture-summarizer repo. This tool utilizes the HuggingFace Pytorch transformers library to run extractive summarizations. This works by first embedding the sentences, then running a clustering algorithm, finding the sentences that are closest to the cluster's centroids. This library also uses coreference techniques, utilizing the https://github.com/huggingface/neuralcoref library to resolve words in summaries that need more context. The greedyness of the neuralcoref library can be tweaked in the SingleModel class.
  - https://github.com/dmmiller612/bert-extractive-summarizer 
  - Leveraging BERT for Extractive Text Summarization on Lectures https://arxiv.org/pdf/1906.04165.pdf

- [textrank_demo](https://github.com/ceshine/textrank_demo) 
  - TextRank Demo - A simple website demonstrating TextRank's extractive summarization capability.
  - A simple website demonstrating TextRank's extractive summarization capability. Currently supports English and Chinese
- [Text-Summarization-with-Amazon-Reviews](https://github.com/Currie32/Text-Summarization-with-Amazon-Reviews)
  - The objective of this project is to build a seq2seq model that can create relevant summaries for reviews written about fine foods sold on Amazon. This dataset contains above 500,000 reviews, and is hosted on [Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews). It's too large to host here, it's over 300MB.





## 趋势,总结

- 一文看尽2019全年AI技术突破

  - https://mp.weixin.qq.com/s/qzt1j5hjFVcLLbJc-MwKpQ

  - 【关键字】AI 2019 突破 

  - 自然语言处理（NLP）：语言模型井喷，部署工具涌现  大型预训练语言模型成常态 新的测试标准推出

    - Transformer
    - GPT-2  https://openai.com/blog/gpt-2-1-5b-release/
    - XLNet 
    - RoBERTa 
    - mBERT
    - ERNIE 2.0
    - SuperGLU https://w4ngatang.github.io/static/papers/superglue.pdf
    - DistilBERT
    - ALBERT

  - 2020趋势

    - 延续当前趋势，在更大的数据集上训练更大的深度学习模型；

      构建更多的生产应用程序，较小的NLP模型将对此有所帮助；

      手动注释文本数据的成本很高，因此半监督标记方法可能会变得很重要；

      NLP模型的可解释性，了解模型在进行公正决策时学到的知识。

    - 延续当前趋势，在更大的数据集上训练更大的深度学习模型；

      构建更多的生产应用程序，较小的NLP模型将对此有所帮助；

      手动注释文本数据的成本很高，因此半监督标记方法可能会变得很重要；

      NLP模型的可解释性，了解模型在进行公正决策时学到的知识。

- 【学术】2020上的NLP有哪些研究风向？

  - https://mp.weixin.qq.com/s/07cD_3ptd3shYWxd-J_YBw
  - 【关键字】AAAI 2020论文预讲会 NLP 
  - **翻译、对话与文本生成**
    - Modeling Fluency and Faithfulness for Diverse Neural Machine Translation
    - Modeling Fluency and Faithfulness for Diverse Neural Machine Translation
    - Neural Machine Translation with Joint Representation
    - Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context
    - A pre-training based personalized dialogue generation model with persona-sparse data
    - SPARQA: Skeleton-based Semantic Parsing for Complex Questions over Knowledge Bases
    - Knowledge Graph Grounded Goal Planning for Open-Domain Conversation Generation
  - **文本分析与内容挖掘**
    - Towards Building a Multilingual Sememe Knowledge Base: Predicting Sememes for BabelNet Synsets
    - Multi-Scale Self-Attention for Text Classification
    - Learning Multi-level Dependencies for Robust Word Recognition
    - Cross-Lingual Low-Resource Set-to-Description Retrieval for Global E-Commerce
    -  Integrating Relation Constraints with Neural Relation Extractors
    - Capturing Sentence Relations for Answer Sentence Selection with Multi-Perspective Graph Encoding
    - Replicate, Walk, and Stop on Syntax: an Effective Neural Network Model for Aspect-Level Sentiment Classification
    - Cross-Lingual Natural Language Generation via Pre-Training
  - **知识理解与NLP应用**
    - Hyperbolic Interaction Model For Hierarchical Multi-Label Classification
    - Multi-channel Reverse Dictionary Model
    - Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement
    - Logo-2K+: A Large-Scale Logo Dataset for Scalable Logo Classification
    - DMRM: A Dual-channel Multi-hop Reasoning Model for Visual Dialog
    - DualVD: An Adaptive Dual Encoding Model for Deep Visual  Understanding  in Visual Dialogue
    - Storytelling from an Image Stream Using Scene Graphs
    - Draft and Edit: Automatic Storytelling Through Multi-Pass Hierarchical Conditional Variational Autoencoder
  - **自然语言中的机器学习**
    - Learning Sparse Sharing Architectures for Multiple Tasks 
    - Reinforcement Learning from Imperfect Demonstrations under Soft Expert Guidance
    - Shapley Q-value: A Local Reward Approach to Solve Global Reward Games
    - Measuring and relieving the over-smoothing problem in graph neural networks from the topological view
    - Neighborhood Cognition Consistent Multi-Agent Reinforcement Learning
    - Neural Snowball for Few-Shot Relation Learning
    - Multi-Task Self-Supervised Learning for Disfluency Detection
    - Constructing Multiple Tasks for Augmentation: Improving Neural Image Classification With K-means Features
    - Graph-propagation based correlation learning for fine-grained image classification
    - End-to-End Bootstrapping Neural Network for Entity Set Expansion

- **文本摘要的“近代史”**

  - http://pfliu.com/Historiography/summarization/summ.html
  - 摘要研究的热度变化
  - 数据仓库
  - 活跃研究小组

- [NLP-progress](https://github.com/sebastianruder/NLP-progress)

  - https://github.com/sebastianruder/NLP-progress/blob/master/english/summarization.md

- Paper reading list in natural language processing.

  - https://github.com/iwangjian/Paper-Reading/blob/master/README.md#text-summarization

- Reading List

  - https://github.com/lipiji/App-DL/blob/master/README.md

    Text Summarization

- https://github.com/mathsyouth/awesome-text-summarization

- https://github.com/icoxfog417/awesome-text-summarization

- https://github.com/bifeng/nlp_paper_notes/blob/75cf64a7eb244814fccf241d5990e23526352ab3/Summarization.md
