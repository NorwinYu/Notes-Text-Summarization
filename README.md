# Notes-Text-Summarization

Date 2020.2
For MLP Course Project with [@Hzlvv](https://github.com/Hzlvv)


## é˜…è¯»

### è®ºæ–‡

Training Language GANs from Scratch https://arxiv.org/pdf/1905.09922.pdf



Towards Explainable NLP: A Generative Explanation Framework for Text Classification https://www.aclweb.org/anthology/P19-1560.pdf



Using Semantic Similarity as Reward for Reinforcement Learning in Sentence Generation https://www.aclweb.org/anthology/P19-2056.pdf



Adversarial Learning for Neural Dialogue Generation https://arxiv.org/pdf/1701.06547.pdf

Sequence-to-Sequence Generative Argumentative Dialogue Systems with Self-Attention https://web.stanford.edu/class/cs224n/reports/custom/15844523.pdf



Generating Logical Forms from Graph Representations of Text and Entities https://www.aclweb.org/anthology/P19-1010.pdf



Synthetic QA Corpora Generation with Roundtrip Consistency https://www.aclweb.org/anthology/P19-1620.pdf



Keeping Notes: Conditional Natural Language Generation with a Scratchpad Mechanism https://arxiv.org/pdf/1906.05275.pdf



This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation https://www.aclweb.org/anthology/P19-1043.pdf

Strategies for Structuring Story Generation https://www.aclweb.org/anthology/P19-1254.pdf

Generating Summaries with Topic Templates and Structured Convolutional Decoders https://www.aclweb.org/anthology/P19-1504.pdf

Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization https://www.aclweb.org/anthology/D18-1206.pdf

HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization https://www.aclweb.org/anthology/P19-1499.pdf

Fine-tune BERT for Extractive Summarization https://arxiv.org/pdf/1903.10318.pdf

Ranking Sentences for Extractive Summarization with Reinforcement Learning https://arxiv.org/pdf/1802.08636.pdf

Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model https://www.aclweb.org/anthology/P19-1102.pdf

A Simple Theoretical Model of Importance for Summarization https://www.aclweb.org/anthology/P19-1101.pdf

Text Summarization with Pretrained Encoders https://www.aclweb.org/anthology/D19-1387.pdf

PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization [Google] https://arxiv.org/pdf/1912.08777.pdf

Leveraging BERT for Extractive Text Summarization on Lectures https://arxiv.org/pdf/1906.04165.pdf

Text Summarization Method based on Double Attention Pointer Network https://www.researchgate.net/publication/338521721_Text_Summarization_Method_based_on_Double_Attention_Pointer_Network

Universal Language Model Fine-tuning for Text Classification https://arxiv.org/pdf/1801.06146.pdf

### å…¶ä»–

#### NLPépaper

- ä¸€ä»½ä»å…¥é—¨åˆ°ç²¾é€šNLPçš„å®Œæ•´æŒ‡å— ï½œ NLPer
  - https://mp.weixin.qq.com/s/AzLvUhN0aUdRIUmUcNUApw
  - ã€å…³é”®å­—ã€‘NLP æŒ‡å— å¤§å…¨

- NLPçš„12ç§åBERTé¢„è®­ç»ƒæ–¹æ³•
  - https://mp.weixin.qq.com/s/mkDmn4zy_s87kiiDIkx0VQ
  - RoBERTa ERNIE ERNIE 2.0 XLMs MASS UNILM CMLM ELECTRA SpanBERT ALBERT  MT-DNN XLENET

- æ¸…åå¤§å­¦NLPç»„12ä»½è®ºæ–‡æ¸…å•å¤§ç¤¼åŒ…é€ç»™ä½ !
  - https://mp.weixin.qq.com/s/Tf131lzPblx6D4oe73Pb0g
  - https://github.com/thunlp/LegalPapers

- å½“æ·±åº¦å­¦ä¹ é‡è§è‡ªåŠ¨æ–‡æœ¬æ‘˜è¦
  - https://cloud.tencent.com/developer/article/1005548
- å¹²è´§ | åŒ—äº¬å¤§å­¦è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ç ”ç©¶æ‰€æ•™æˆä¸‡å°å†›ï¼šæ–‡æœ¬è‡ªåŠ¨æ‘˜è¦æŠ€æœ¯
  -  https://mp.weixin.qq.com/s?__biz=MzAxMzc2NDAxOQ==&mid=2650363002&idx=2&sn=8c1f28d2944a373ae9133cb29bcd4c07
- çŸ¥è¯†å›¾è°±å¦‚ä½•åŠ©åŠ›æ–‡æœ¬æ‘˜è¦ç”Ÿæˆ
  - https://mp.weixin.qq.com/s/TR9db8wpU7ooqZ6acKmrow
- Google Brainâ€™s AI achieves state-of-the-art text summarization performance
  - https://venturebeat.com/2019/12/23/google-brains-ai-achieves-state-of-the-art-text-summarization-performance/
- The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) https://jalammar.github.io/illustrated-bert/
- è¿ç§»å­¦ä¹ NLPï¼šBERTã€ELMoç­‰ç›´è§‚å›¾è§£
  - https://zhuanlan.zhihu.com/p/52282552
- ã€èµ„æºæ¨èã€‘BERTè®ºæ–‡åˆé›†
  - https://mp.weixin.qq.com/s/29Esivnb7sfjLT82sqR1Nw

#### å¼ºåŒ–å­¦ä¹ 

- å¼ºåŒ–å­¦ä¹ åœ¨é˜¿é‡Œçš„æŠ€æœ¯æ¼”è¿›ä¸ä¸šåŠ¡åˆ›æ–° http://techforum-img.cn-hangzhou.oss-pub.aliyun-inc.com/1517812754285/reinforcement_learning.pdf

- RL in NMT: The Good, the Bad and the Ugly https://www.cl.uni-heidelberg.de/statnlpgroup/blog/rl4nmt/

- æœºå™¨ç¿»è¯‘ä¸­çš„å¼ºåŒ–å­¦ä¹ ï¼šä¼˜ç‚¹ã€ç¼ºç‚¹ä»¥åŠä¸è¶³ https://zhuanlan.zhihu.com/p/101324828

- Reinforcement Learning for NLP https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture16-guest.pdf

- ç‚¼ä¸¹æ„Ÿæ‚Ÿï¼šOn the Generalization of RL https://zhuanlan.zhihu.com/p/105898705?utm_source=wechat_session&utm_medium=social&utm_oi=886328359599079424&wechatShare=1&s_s_i=6R6qXGoP1moxUv142VK6VTjNDy1osnvZhH5tUj9kQ5Q%3D&s_r=0&from=groupmessage&isappinstalled=0

####  æ¨¡å‹å‹ç¼©

- AdderNet: Do We Really Need Multiplications in Deep Learning? https://github.com/huawei-noah/AdderNet

## æ•°æ®é›†ã€ä»»åŠ¡

#### WIT

- WIT3 - acronym for Web Inventory of Transcribed and Translated Talks - is a ready-to-use version for research purposes of the multilingual transcriptions of [TED](http://www.ted.com/) talks
- https://wit3.fbk.eu/
- http://www.mt-archive.info/EAMT-2012-Cettolo.pdf

#### Multi30k

- Multi30K: Multilingual English-German Image Descriptions

- https://arxiv.org/pdf/1605.00459.pdf
- https://github.com/multi30k/dataset

#### Frames Dataset

- https://www.microsoft.com/en-us/research/project/frames-dataset/?secret=ZyeOOd#!stats
- The dialogues in Frames were collected in a Wizard-of-Oz fashion. Two humans talked to each other via a chat interface. 



#### The Blog Authorship Corpus 

- The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.  

  Each blog is presented as a separate file, the name of which indicates a blogger id# and the bloggerâ€™s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)

- http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm

- Effects of Age and Gender on Blogging http://u.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf

#### The Project Gutenberg collection

- The Project Gutenberg collection is available from dozens of sites offering access via http/https, ftp and/or rsync. See our listing of [mirror sites](https://www.gutenberg.org/MIRRORS.ALL) to choose the location closest to you. A close location may give you faster downloads. Mirrors generally do not have a friendly Web-based front end, but do have the collection. See the [mirroring how-to](https://www.gutenberg.org/wiki/Gutenberg:Mirroring_How-To) for details.
- https://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs

#### Enron

- This dataset was collected and prepared by the [CALO Project](http://www.ai.sri.com/project/CALO) (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 0.5M messages. This data was originally [made public, and posted to the web](http://www.salon.com/news/feature/2003/10/14/enron/index_np.html), by the [Federal Energy Regulatory Commission](http://www.ferc.gov/) during its investigation.
- https://www.cs.cmu.edu/~enron/

#### NEWSROOM

- **CORNELL NEWSROOM** is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017 and use a variety of summarization strategies combining *extraction* and *abstraction*.

- https://summari.es/
- https://github.com/lil-lab/newsroom
- [Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies](https://www.aclweb.org/anthology/N18-1065.pdf)

#### WikiHow

- A Large Scale Text Summarization Dataset
- WikiHow is a new large-scale dataset using the online WikiHow (http://www.wikihow.com/) knowledge base [*](https://github.com/mahnazkoupaee/WikiHow-Dataset#footnote1). 
- https://arxiv.org/abs/1810.09305

#### Reader-Aware Multi-Document Summarization (RA-MDS)

- Traditional multi-document summarization aims at generating a summary from a set of text documents, e.g., news, of a given topic/event. We investigate a problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.
- http://www1.se.cuhk.edu.hk/~textmine/dataset/ra-mds/

### ä¸­æ–‡

#### LCSTS

- Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, we introduce a **L**arge-scale **C**hinese **S**hort **T**ext **S**ummarization dataset constructed from the Chinese microblogging website SinaWeibo. This corpus consists of over 2 million real Chinese short texts with short summaries given by the writer of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.

- [LCSTS: A Large Scale Chinese Short Text Summarization Dataset](https://www.aclweb.org/anthology/D15-1229.pdf)
- http://icrc.hitsz.edu.cn/Article/show/139.html

#### **NLPCC 2017 Shared Task**

-  **Single Document Summarization**
- http://tcci.ccf.org.cn/conference/2017/taskdata.php

#### æ•™è‚²åŸ¹è®­è¡Œä¸šæŠ½è±¡å¼è‡ªåŠ¨æ‘˜è¦ä¸­æ–‡è¯­æ–™åº“

- æ•™è‚²è¡Œä¸šæ–°é—» è‡ªåŠ¨æ–‡æ‘˜ è¯­æ–™åº“ è‡ªåŠ¨æ‘˜è¦
- https://github.com/wonderfulsuccess/chinese_abstractive_corpus

#### chinese-poetry: æœ€å…¨ä¸­æ–‡è¯—æ­Œå¤å…¸æ–‡é›†æ•°æ®åº“

- æœ€å…¨çš„ä¸­åå¤å…¸æ–‡é›†æ•°æ®åº“ï¼ŒåŒ…å« 5.5 ä¸‡é¦–å”è¯—ã€26 ä¸‡é¦–å®‹è¯—ã€2.1 ä¸‡é¦–å®‹è¯å’Œå…¶ä»–å¤å…¸æ–‡é›†ã€‚è¯—äººåŒ…æ‹¬å”å®‹ä¸¤æœè¿‘ 1.4 ä¸‡å¤è¯—äººï¼Œå’Œä¸¤å®‹æ—¶æœŸ 1.5 åƒå¤è¯äººã€‚æ•°æ®æ¥æºäºäº’è”ç½‘ã€‚
- The most comprehensive database of Chinese poetry ğŸ§¶æœ€å…¨ä¸­åå¤è¯—è¯æ•°æ®åº“, å”å®‹ä¸¤æœè¿‘ä¸€ä¸‡å››åƒå¤è¯—äºº, æ¥è¿‘5.5ä¸‡é¦–å”è¯—åŠ 26ä¸‡å®‹è¯—. ä¸¤å®‹æ—¶æœŸ1564ä½è¯äººï¼Œ21050é¦–è¯ã€‚
- [http://shici.store](http://shici.store/)
- https://github.com/chinese-poetry/chinese-poetry

### Source

æ•°æ®é›† - é˜¿é‡Œç³»å”¯ä¸€å¯¹å¤–å¼€æ”¾æ•°æ®åˆ†äº«å¹³å° https://tianchi.aliyun.com/dataset/?spm=5176.12281976.0.0.79166b2dgfWglv

Open Data on AWS https://registry.opendata.aws/

[ParlAI](https://github.com/facebookresearch/ParlAI) A framework for training and evaluating AI models on a variety of openly available dialogue dataset https://parl.ai/

NLDS Corpora https://nlds.soe.ucsc.edu/software [Natural Language and Dialogue Systems](https://nlds.soe.ucsc.edu/

[nlp-datasets](https://github.com/niderhoff/nlp-datasets)

ä¸­æ–‡é•¿æ–‡æœ¬æ‘˜è¦æœ‰å“ªäº›æ•°æ®é›†ï¼Ÿ https://www.zhihu.com/question/306887936

ä¸­æ–‡æ•°æ®é›†æœç´¢ - æ–‡æœ¬æ‘˜è¦ https://www.cluebenchmarks.com/dataSet_search_modify.html?keywords=%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81





## æ¨¡å‹

#### XLNet

- XLNet: Generalized Autoregressive Pretraining for Language Understanding

- https://github.com/zihangdai/xlnet
- https://arxiv.org/pdf/1906.08237.pdf
- ã€ä»€ä¹ˆæ˜¯XLNetï¼Œå®ƒä¸ºä»€ä¹ˆæ¯”BERTæ•ˆæœå¥½ã€‘https://zhuanlan.zhihu.com/p/107350079?utm_source=wechat_session&utm_medium=social&utm_oi=610548841527775232&s_r=0&from=groupmessage&isappinstalled=0
- ã€20é¡¹ä»»åŠ¡ä¸Šç¢¾å‹BERTï¼ŒCMUå’ŒGoogleå¤§è„‘è”åˆæ¨å‡ºXLNetæ¨¡å‹ï¼ˆå…¨æ–‡ç¿»è¯‘ã€‘https://mp.weixin.qq.com/s/rPhHwg2fILT28X2sdPH56A

#### ALBERT

- ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS

- https://github.com/google-research/ALBERT
- https://openreview.net/attachment?id=H1eA7AEtvS&name=original_pdf
- ã€å…¨é¢éœ¸æ¦œï¼Googleæ¨å‡ºè½»é‡çº§BERTæ¨¡å‹ï¼šALBERTï¼ˆå…¨æ–‡ç¿»è¯‘ï¼‰ã€‘https://mp.weixin.qq.com/s/vuk8yK9E_DuWuoWJs3cijw

#### Text-To-Text Transfer Transformer - T5

- https://github.com/google-research/text-to-text-transfer-transformer
- https://arxiv.org/pdf/1910.10683.pdf
- ã€çªç ´è¿ç§»å­¦ä¹ å±€é™ï¼è°·æ­Œæå‡ºâ€œT5â€ æ–°NLPæ¨¡å‹ï¼Œå¤šåŸºå‡†æµ‹è¯•è¾¾SOTAã€‘https://mp.weixin.qq.com/s/p3Mg5MKiCLaYQOWQ7NtcNA

#### ULMFiT

- Universal Language Model Fine-tuning for Text Classification https://arxiv.org/pdf/1801.06146.pdf
- ã€2018è¿ç§»å­¦ä¹ ä¼˜ç§€è®ºæ–‡-ULMFiT-é¢å‘æ–‡æœ¬åˆ†ç±»çš„é¢„è®­ç»ƒæ¨¡å‹ã€‘https://zhuanlan.zhihu.com/p/55348816

#### NEZHA

- NEZHA: NEURAL CONTEXTUALIZED REPRESENTATION FOR CHINESE LANGUAGE UNDERSTANDING
- åä¸ºåŸºäºbertæå‡ºçš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹NEZHAä¸­æå‡ºä¸€ç§æ–°çš„å‡½æ•°å‹çš„ç›¸å¯¹ä½ç½®çš„position encodingçš„æ–¹æ³•ï¼Œå¹¶ä¸”è¯´æ¯”ç›®å‰çš„bertçš„æ–¹å¼æ›´ä¼˜
- https://arxiv.org/pdf/1909.00204.pdf

#### UNILM

- Microsoft Research

- Unified Language Model Pre-training for Natural Language Understanding and Generation
- https://arxiv.org/pdf/1905.03197.pdf

- https://github.com/microsoft/unilm

### Text Summarization

#### Xsum

- Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization https://www.aclweb.org/anthology/D18-1206.pdf

- https://github.com/EdinburghNLP/XSum
- https://vimeo.com/305885893

#### HIBERT

- HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization
- https://www.aclweb.org/anthology/P19-1499.pdf
- https://xingxingzhang.github.io/hibert.html

#### Presumm

- https://github.com/nlpyang/PreSumm
- Text Summarization with Pretrained Encoders https://www.aclweb.org/anthology/D19-1387.pdf
- Fine-tune BERT for Extractive Summarization https://arxiv.org/pdf/1903.10318.pdf



## æ¡†æ¶,å®ç°,å·¥å…·

- [dialogue-seq2seq](https://github.com/vliu15/dialogue-seq2seq)

  - Sequence-to-Sequence Generative Dialogue Systems. This is a Pytorch adaptation of the Transformer model in "[Attention is All You Need](https://arxiv.org/abs/1706.03762)" for memory-based generative dialogue systems. We borrow the Transformer encoder and decoder to encode decode individual responses. The encoded input updates a hidden state in an LSTM, which serves as a session memory. We train our dialogue system with the "[Internet Argument Corpus v1](https://nlds.soe.ucsc.edu/iac)".

- [bertviz](https://github.com/jessevig/bertviz)

  - BertViz is a tool for visualizing attention in the Transformer model, supporting all models from the [transformers](https://github.com/huggingface/transformers) library (BERT, GPT-2, XLNet, RoBERTa, XLM, CTRL, etc.). It extends the [Tensor2Tensor visualization tool](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization) by [Llion Jones](https://medium.com/@llionj) and the [transformers](https://github.com/huggingface/transformers) library from [HuggingFace](https://github.com/huggingface).

- [BERT-pytorch](https://github.com/codertimo/BERT-pytorch)

  - Pytorch implementation of Google AI's 2018 BERT, with simple annotation

- [transformers](https://github.com/huggingface/transformers)

  - ğŸ¤— Transformers (formerly known as `pytorch-transformers` and `pytorch-pretrained-bert`) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.

  - ğŸ¤— Transformers currently provides the following NLU/NLG architectures:

    1. **BERT** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.
    2. **GPT** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.
    3. **GPT-2** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.
    4. **Transformer-XL** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.
    5. **XLNet** (from Google/CMU) released with the paper [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.
    6. **XLM** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.
    7. **RoBERTa** (from Facebook), released together with the paper a [Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.
    8. **DistilBERT** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/master/examples/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/master/examples/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/master/examples/distillation) and a German version of DistilBERT.
    9. **CTRL** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.
    10. **CamemBERT** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz SuÃ¡rez*, Yoann Dupont, Laurent Romary, Ã‰ric Villemonte de la Clergerie, DjamÃ© Seddah and BenoÃ®t Sagot.
    11. **ALBERT** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.
    12. **T5** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.
    13. **XLM-RoBERTa** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.
    14. **MMBT** (from Facebook), released together with the paper a [Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/pdf/1909.02950.pdf) by Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Davide Testuggine.
    15. **FlauBERT** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, LoÃ¯c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, BenoÃ®t CrabbÃ©, Laurent Besacier, Didier Schwab.
    16. **Other community models**, contributed by the [community](https://huggingface.co/users).
    17. Want to contribute a new model? We have added a **detailed guide and templates** to guide you in the process of adding a new model. You can find them in the [`templates`](https://github.com/huggingface/transformers/blob/master/templates) folder of the repository. Be sure to check the [contributing guidelines](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md) and contact the maintainers or open an issue to collect feedbacks before starting your PR.

    These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the [documentation](https://huggingface.co/transformers/examples.html).

- Bert Extractive Summarizer 

  - This repo is the generalization of the lecture-summarizer repo. This tool utilizes the HuggingFace Pytorch transformers library to run extractive summarizations. This works by first embedding the sentences, then running a clustering algorithm, finding the sentences that are closest to the cluster's centroids. This library also uses coreference techniques, utilizing the https://github.com/huggingface/neuralcoref library to resolve words in summaries that need more context. The greedyness of the neuralcoref library can be tweaked in the SingleModel class.
  - https://github.com/dmmiller612/bert-extractive-summarizer 
  - Leveraging BERT for Extractive Text Summarization on Lectures https://arxiv.org/pdf/1906.04165.pdf

- [textrank_demo](https://github.com/ceshine/textrank_demo) 
  - TextRank Demo - A simple website demonstrating TextRank's extractive summarization capability.
  - A simple website demonstrating TextRank's extractive summarization capability. Currently supports English and Chinese
- [Text-Summarization-with-Amazon-Reviews](https://github.com/Currie32/Text-Summarization-with-Amazon-Reviews)
  - The objective of this project is to build a seq2seq model that can create relevant summaries for reviews written about fine foods sold on Amazon. This dataset contains above 500,000 reviews, and is hosted on [Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews). It's too large to host here, it's over 300MB.





## è¶‹åŠ¿,æ€»ç»“

- ä¸€æ–‡çœ‹å°½2019å…¨å¹´AIæŠ€æœ¯çªç ´

  - https://mp.weixin.qq.com/s/qzt1j5hjFVcLLbJc-MwKpQ

  - ã€å…³é”®å­—ã€‘AI 2019 çªç ´ 

  - è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼šè¯­è¨€æ¨¡å‹äº•å–·ï¼Œéƒ¨ç½²å·¥å…·æ¶Œç°  å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æˆå¸¸æ€ æ–°çš„æµ‹è¯•æ ‡å‡†æ¨å‡º

    - Transformer
    - GPT-2  https://openai.com/blog/gpt-2-1-5b-release/
    - XLNet 
    - RoBERTa 
    - mBERT
    - ERNIE 2.0
    - SuperGLU https://w4ngatang.github.io/static/papers/superglue.pdf
    - DistilBERT
    - ALBERT

  - 2020è¶‹åŠ¿

    - å»¶ç»­å½“å‰è¶‹åŠ¿ï¼Œåœ¨æ›´å¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ›´å¤§çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼›

      æ„å»ºæ›´å¤šçš„ç”Ÿäº§åº”ç”¨ç¨‹åºï¼Œè¾ƒå°çš„NLPæ¨¡å‹å°†å¯¹æ­¤æœ‰æ‰€å¸®åŠ©ï¼›

      æ‰‹åŠ¨æ³¨é‡Šæ–‡æœ¬æ•°æ®çš„æˆæœ¬å¾ˆé«˜ï¼Œå› æ­¤åŠç›‘ç£æ ‡è®°æ–¹æ³•å¯èƒ½ä¼šå˜å¾—å¾ˆé‡è¦ï¼›

      NLPæ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œäº†è§£æ¨¡å‹åœ¨è¿›è¡Œå…¬æ­£å†³ç­–æ—¶å­¦åˆ°çš„çŸ¥è¯†ã€‚

    - å»¶ç»­å½“å‰è¶‹åŠ¿ï¼Œåœ¨æ›´å¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ›´å¤§çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼›

      æ„å»ºæ›´å¤šçš„ç”Ÿäº§åº”ç”¨ç¨‹åºï¼Œè¾ƒå°çš„NLPæ¨¡å‹å°†å¯¹æ­¤æœ‰æ‰€å¸®åŠ©ï¼›

      æ‰‹åŠ¨æ³¨é‡Šæ–‡æœ¬æ•°æ®çš„æˆæœ¬å¾ˆé«˜ï¼Œå› æ­¤åŠç›‘ç£æ ‡è®°æ–¹æ³•å¯èƒ½ä¼šå˜å¾—å¾ˆé‡è¦ï¼›

      NLPæ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œäº†è§£æ¨¡å‹åœ¨è¿›è¡Œå…¬æ­£å†³ç­–æ—¶å­¦åˆ°çš„çŸ¥è¯†ã€‚

- ã€å­¦æœ¯ã€‘2020ä¸Šçš„NLPæœ‰å“ªäº›ç ”ç©¶é£å‘ï¼Ÿ

  - https://mp.weixin.qq.com/s/07cD_3ptd3shYWxd-J_YBw
  - ã€å…³é”®å­—ã€‘AAAI 2020è®ºæ–‡é¢„è®²ä¼š NLP 
  - **ç¿»è¯‘ã€å¯¹è¯ä¸æ–‡æœ¬ç”Ÿæˆ**
    - Modeling Fluency and Faithfulness for Diverse Neural Machine Translation
    - Modeling Fluency and Faithfulness for Diverse Neural Machine Translation
    - Neural Machine Translation with Joint Representation
    - Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context
    - A pre-training based personalized dialogue generation model with persona-sparse data
    - SPARQA: Skeleton-based Semantic Parsing for Complex Questions over Knowledge Bases
    - Knowledge Graph Grounded Goal Planning for Open-Domain Conversation Generation
  - **æ–‡æœ¬åˆ†æä¸å†…å®¹æŒ–æ˜**
    - Towards Building a Multilingual Sememe Knowledge Base: Predicting Sememes for BabelNet Synsets
    - Multi-Scale Self-Attention for Text Classification
    - Learning Multi-level Dependencies for Robust Word Recognition
    - Cross-Lingual Low-Resource Set-to-Description Retrieval for Global E-Commerce
    -  Integrating Relation Constraints with Neural Relation Extractors
    - Capturing Sentence Relations for Answer Sentence Selection with Multi-Perspective Graph Encoding
    - Replicate, Walk, and Stop on Syntax: an Effective Neural Network Model for Aspect-Level Sentiment Classification
    - Cross-Lingual Natural Language Generation via Pre-Training
  - **çŸ¥è¯†ç†è§£ä¸NLPåº”ç”¨**
    - Hyperbolic Interaction Model For Hierarchical Multi-Label Classification
    - Multi-channel Reverse Dictionary Model
    - Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement
    - Logo-2K+: A Large-Scale Logo Dataset for Scalable Logo Classification
    - DMRM: A Dual-channel Multi-hop Reasoning Model for Visual Dialog
    - DualVD: An Adaptive Dual Encoding Model for Deep Visual  Understanding  in Visual Dialogue
    - Storytelling from an Image Stream Using Scene Graphs
    - Draft and Edit: Automatic Storytelling Through Multi-Pass Hierarchical Conditional Variational Autoencoder
  - **è‡ªç„¶è¯­è¨€ä¸­çš„æœºå™¨å­¦ä¹ **
    - Learning Sparse Sharing Architectures for Multiple Tasks 
    - Reinforcement Learning from Imperfect Demonstrations under Soft Expert Guidance
    - Shapley Q-value: A Local Reward Approach to Solve Global Reward Games
    - Measuring and relieving the over-smoothing problem in graph neural networks from the topological view
    - Neighborhood Cognition Consistent Multi-Agent Reinforcement Learning
    - Neural Snowball for Few-Shot Relation Learning
    - Multi-Task Self-Supervised Learning for Disfluency Detection
    - Constructing Multiple Tasks for Augmentation: Improving Neural Image Classification With K-means Features
    - Graph-propagation based correlation learning for fine-grained image classification
    - End-to-End Bootstrapping Neural Network for Entity Set Expansion

- **æ–‡æœ¬æ‘˜è¦çš„â€œè¿‘ä»£å²â€**

  - http://pfliu.com/Historiography/summarization/summ.html
  - æ‘˜è¦ç ”ç©¶çš„çƒ­åº¦å˜åŒ–
  - æ•°æ®ä»“åº“
  - æ´»è·ƒç ”ç©¶å°ç»„

- [NLP-progress](https://github.com/sebastianruder/NLP-progress)

  - https://github.com/sebastianruder/NLP-progress/blob/master/english/summarization.md

- Paper reading list in natural language processing.

  - https://github.com/iwangjian/Paper-Reading/blob/master/README.md#text-summarization

- Reading List

  - https://github.com/lipiji/App-DL/blob/master/README.md

    Text Summarization

- https://github.com/mathsyouth/awesome-text-summarization

- https://github.com/icoxfog417/awesome-text-summarization

- https://github.com/bifeng/nlp_paper_notes/blob/75cf64a7eb244814fccf241d5990e23526352ab3/Summarization.md
